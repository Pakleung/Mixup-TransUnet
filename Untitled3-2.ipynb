{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9doMj_n2pgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78921813-5435-484b-f05e-e4b47f175475"
      },
      "source": [
        "pip install einops"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrbuPGs8Wiu9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLN5yMbB2rlO"
      },
      "source": [
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=None):\n",
        "        \"\"\"\n",
        "        Implementation of multi-head attention layer of the original transformer model.\n",
        "        einsum and einops.rearrange is used whenever possible\n",
        "        Args:\n",
        "            dim: token's dimension, i.e. word embedding vector size\n",
        "            heads: the number of distinct representations to learn\n",
        "            dim_head: the dim of the head. In general dim_head<dim.\n",
        "            However, it may not necessary be (dim/heads)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
        "        _dim = self.dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.to_qvk = nn.Linear(dim, _dim * 3, bias=False)\n",
        "        self.W_0 = nn.Linear(_dim, dim, bias=False)\n",
        "        self.scale_factor = self.dim_head ** -0.5\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        assert x.dim() == 3\n",
        "        qkv = self.to_qvk(x)  # [batch, tokens, dim*3*heads ]\n",
        "\n",
        "        # decomposition to q,v,k and cast to tuple\n",
        "        # the resulted shape before casting to tuple will be: [3, batch, heads, tokens, dim_head]\n",
        "        q, k, v = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.heads))\n",
        "\n",
        "        out = compute_mhsa(q, k, v, mask=mask, scale_factor=self.scale_factor)\n",
        "\n",
        "        # re-compose: merge heads with dim_head\n",
        "        out = rearrange(out, \"b h t d -> b t (h d)\")\n",
        "        # Apply final linear transformation layer\n",
        "        return self.W_0(out)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56lYuML6WO5w"
      },
      "source": [
        "def compute_mhsa(q, k, v, scale_factor=1, mask=None):\n",
        "    # resulted shape will be: [batch, heads, tokens, tokens]\n",
        "    scaled_dot_prod = torch.einsum('... i d , ... j d -> ... i j', q, k) * scale_factor\n",
        "\n",
        "    if mask is not None:\n",
        "        assert mask.shape == scaled_dot_prod.shape[2:]\n",
        "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "    # calc result per head\n",
        "    return torch.einsum('... i j , ... j d -> ... i d', attention, v)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMRkS7Sx2egq"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Vanilla transformer block from the original paper \"Attention is all you need\"\n",
        "    Detailed analysis: https://theaisummer.com/transformer/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, heads=8, dim_head=None,\n",
        "                 dim_linear_block=1024, dropout=0.1, activation=nn.GELU,\n",
        "                 mhsa=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: token's vector length\n",
        "            heads: number of heads\n",
        "            dim_head: if none dim/heads is used\n",
        "            dim_linear_block: the inner projection dim\n",
        "            dropout: probability of droppping values\n",
        "            mhsa: if provided you can change the vanilla self-attention block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mhsa = mhsa if mhsa is not None else MultiHeadSelfAttention(dim=dim, heads=heads, dim_head=dim_head)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.norm_1 = nn.LayerNorm(dim)\n",
        "        self.norm_2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(dim, dim_linear_block),\n",
        "            activation(),  # nn.ReLU or nn.GELU\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_linear_block, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        y = self.norm_1(self.drop(self.mhsa(x, mask)) + x)\n",
        "        return self.norm_2(self.linear(y) + y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YktBVC3ZoP24"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim, blocks=6, heads=8, dim_head=None, dim_linear_block=1024, dropout=0):\n",
        "        super().__init__()\n",
        "        self.block_list = [TransformerBlock(dim, heads, dim_head, dim_linear_block, dropout) for _ in range(blocks)]\n",
        "        self.layers = nn.ModuleList(self.block_list)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxhb-9C1pYdi"
      },
      "source": [
        "from einops import repeat\n",
        "def expand_to_batch(tensor, desired_size):\n",
        "    tile = desired_size // tensor.shape[0]\n",
        "    return repeat(tensor, 'b ... -> (b tile) ...', tile=tile)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTsRjmMQ2zv5"
      },
      "source": [
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *,\n",
        "                 img_dim,\n",
        "                 in_channels=3,\n",
        "                 patch_dim=16,\n",
        "                 num_classes=10,\n",
        "                 dim=512,\n",
        "                 blocks=6,\n",
        "                 heads=4,\n",
        "                 dim_linear_block=1024,\n",
        "                 dim_head=None,\n",
        "                 dropout=0, transformer=None, classification=True):\n",
        "        \"\"\"\n",
        "        Minimal re-implementation of ViT\n",
        "        Args:\n",
        "            img_dim: the spatial image size\n",
        "            in_channels: number of img channels\n",
        "            patch_dim: desired patch dim\n",
        "            num_classes: classification task classes\n",
        "            dim: the linear layer's dim to project the patches for MHSA\n",
        "            blocks: number of transformer blocks\n",
        "            heads: number of heads\n",
        "            dim_linear_block: inner dim of the transformer linear block\n",
        "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
        "            dropout: for pos emb and transformer\n",
        "            transformer: in case you want to provide another transformer implementation\n",
        "            classification: creates an extra CLS token that we will index in the final classification layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible by img dim {img_dim}'\n",
        "        self.p = patch_dim\n",
        "        self.classification = classification\n",
        "        # tokens = number of patches\n",
        "        tokens = (img_dim // patch_dim) ** 2\n",
        "        self.token_dim = in_channels * (patch_dim ** 2)\n",
        "        self.dim = dim\n",
        "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
        "\n",
        "        # Projection and pos embeddings\n",
        "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
        "\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "        if transformer is None:\n",
        "            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
        "                                                  dim_head=self.dim_head,\n",
        "                                                  dim_linear_block=dim_linear_block,\n",
        "                                                  dropout=dropout)\n",
        "        else:\n",
        "            self.transformer = transformer\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        # Create patches\n",
        "        # from [batch, channels, h, w] to [batch, tokens , N], N=p*p*c , tokens = h/p *w/p\n",
        "        img_patches = rearrange(img,\n",
        "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
        "                                patch_x=self.p, patch_y=self.p)\n",
        "\n",
        "        batch_size, tokens, _ = img_patches.shape\n",
        "\n",
        "        # project patches with linear layer + add pos emb\n",
        "        img_patches = self.project_patches(img_patches)\n",
        "\n",
        "        img_patches = torch.cat((expand_to_batch(self.cls_token, desired_size=batch_size), img_patches), dim=1)\n",
        "\n",
        "        # add pos. embeddings. + dropout\n",
        "        # indexing with the current batch's token length to support variable sequences\n",
        "        img_patches = img_patches + self.pos_emb1D[:tokens + 1, :]\n",
        "        patch_embeddings = self.emb_dropout(img_patches)\n",
        "\n",
        "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
        "        y = self.transformer(patch_embeddings, mask)\n",
        "\n",
        "        # we index only the cls token for classification. nlp tricks :P\n",
        "        return self.mlp_head(y[:, 0, :]) if self.classification else y[:, 1:, :]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYtWMwJnCWnQ"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        if stride != 1 or inplanes != planes * self.expansion:\n",
        "            self.downsample = nn.Sequential(\n",
        "                conv1x1(inplanes, planes * self.expansion, stride),\n",
        "                norm_layer(planes * self.expansion),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLUKFf6Q3I8k"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Double convolution block that keeps that spatial sizes the same\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, out_ch, norm_layer=None):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(SignleConv(in_ch, out_ch, norm_layer),\n",
        "                                  SignleConv(out_ch, out_ch, norm_layer))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlQsjgrf0emo"
      },
      "source": [
        "class Up(nn.Module):\n",
        "    \"\"\"\n",
        "    Doubles spatial size with bilinear upsampling\n",
        "    Skip connections and double convs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Up, self).__init__()\n",
        "        mode = \"bilinear\"\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x1: [b,c, h, w]\n",
        "            x2: [b,c, 2*h,2*w]\n",
        "        Returns: 2x upsampled double conv reselt\n",
        "        \"\"\"\n",
        "        x = self.up(x1)\n",
        "        if x2 is not None:\n",
        "            x = torch.cat([x2, x], dim=1)\n",
        "        return self.conv(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVPfn02D1c5L"
      },
      "source": [
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gpMyga1mN8"
      },
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmUk4gOD0ptB"
      },
      "source": [
        "class SignleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Double convolution block that keeps that spatial sizes the same\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, out_ch, norm_layer=None):\n",
        "        super(SignleConv, self).__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
        "            norm_layer(out_ch),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sdb_kUV0Qln"
      },
      "source": [
        "class TransUnet(nn.Module):\n",
        "    def __init__(self, *, img_dim, in_channels, classes,\n",
        "                 vit_blocks=12,\n",
        "                 vit_heads=4,\n",
        "                 vit_dim_linear_mhsa_block=1024,\n",
        "                 vit_transformer=None,\n",
        "                 vit_channels = None\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        My reimplementation of TransUnet based on the paper:\n",
        "        https://arxiv.org/abs/2102.04306\n",
        "        Badly written, many details missing and significantly differently\n",
        "        from the authors official implementation (super messy code also :P ).\n",
        "        My implementation doesnt match 100 the authors code.\n",
        "        Basically I wanted to see the logic with vit and resnet backbone for\n",
        "        shaping a unet model with long skip connections.\n",
        "        Args:\n",
        "            img_dim: the img dimension\n",
        "            in_channels: channels of the input\n",
        "            classes: desired segmentation classes\n",
        "            vit_blocks: MHSA blocks of ViT\n",
        "            vit_heads: number of MHSA heads\n",
        "            vit_dim_linear_mhsa_block: MHSA MLP dimension\n",
        "            vit_transformer: pass your own version of vit\n",
        "            vit_channels: the channels of your pretrained vit. default is 128*8\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.inplanes = 128\n",
        "        vit_channels = self.inplanes * 8 if vit_channels is None else vit_channels\n",
        "\n",
        "        # Not clear how they used resnet arch. since the first input after conv\n",
        "        # must be 128 channels and half spat dims.\n",
        "        in_conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                             bias=False)\n",
        "        bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.init_conv = nn.Sequential(in_conv1, bn1, nn.ReLU(inplace=True))\n",
        "        self.conv1 = Bottleneck(self.inplanes, self.inplanes * 2, stride=2)\n",
        "        self.conv2 = Bottleneck(self.inplanes * 2, self.inplanes * 4, stride=2)\n",
        "        self.conv3 = Bottleneck(self.inplanes * 4, vit_channels, stride=2)\n",
        "\n",
        "        self.img_dim_vit = img_dim // 16\n",
        "\n",
        "        self.vit = ViT(img_dim=self.img_dim_vit,\n",
        "                       in_channels=vit_channels,  # encoder channels\n",
        "                       patch_dim=1,\n",
        "                       dim=vit_channels,  # vit out channels for decoding\n",
        "                       blocks=vit_blocks,\n",
        "                       heads=vit_heads,\n",
        "                       dim_linear_block=vit_dim_linear_mhsa_block,\n",
        "                       classification=False) if vit_transformer is None else vit_transformer\n",
        "\n",
        "        self.vit_conv = SignleConv(in_ch=vit_channels, out_ch=512)\n",
        "\n",
        "        self.dec1 = Up(1024, 256)\n",
        "        self.dec2 = Up(512, 128)\n",
        "        self.dec3 = Up(256, 64)\n",
        "        self.dec4 = Up(64, 16)\n",
        "        self.conv1x1 = nn.Conv2d(16, classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ResNet 50-like encoder\n",
        "        x2 = self.init_conv(x)  # 128,64,64\n",
        "        x4 = self.conv1(x2)  # 256,32,32\n",
        "        x8 = self.conv2(x4)  # 512,16,16\n",
        "        x16 = self.conv3(x8)  # 1024,8,8\n",
        "        y = self.vit(x16)\n",
        "        y = rearrange(y, 'b (x y) dim -> b dim x y ', x=self.img_dim_vit, y=self.img_dim_vit)\n",
        "        y = self.vit_conv(y)\n",
        "        y = self.dec1(y, x8)  # 256,16,16\n",
        "        y = self.dec2(y, x4)\n",
        "        y = self.dec3(y, x2)\n",
        "        y = self.dec4(y)\n",
        "        return self.conv1x1(y)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yye9F2g8RS0"
      },
      "source": [
        "import torch\n",
        "#from self_attention_cv.transunet import TransUnet\n",
        "a = torch.rand(2, 3, 128, 128)\n",
        "model = TransUnet(in_channels=3, img_dim=128, vit_blocks=8,\n",
        "vit_dim_linear_mhsa_block=512, classes=5)\n",
        "y = model(a)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siqyMFbn0Pyf",
        "outputId": "0f526fdc-32dc-48e5-8431-2a2aaa69c693"
      },
      "source": [
        "a = torch.rand(2, 3, 128, 128)\n",
        "\n",
        "model = TransUnet(in_channels=3, img_dim=128, vit_blocks=1, vit_dim_linear_mhsa_block=512, classes=5)\n",
        "y = model(a)\n",
        "print('final out shape:', y.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final out shape: torch.Size([2, 5, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdixzvXbW1KJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}